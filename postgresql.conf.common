shared_preload_libraries = 'pg_pathman, pg_shardman'

# We support printing node ids in log messages. For that, include '%z' in
# log_line_prefix. This feature is only supported if there is one shardman
# installation per database cluster.
log_line_prefix = '%m %z'

log_min_messages = DEBUG1
client_min_messages = NOTICE
log_replication_commands = on

# We use logical replication for sending metadata from shardlord to workers
# and for data replication.
wal_level = logical # necessary for logical replication

# On shardlord, this must be at least max number of worker nodes + some reserved
# for initial tablesync.
#
# On worker node 'A', this must be at least 'mrs':
# mrs = 1
# for each sharded table t
#   for each primary shard of t lying on A
#     mrs += number of this shard replicas
#   for each replica shard of t lying on A
#     mrs++
#
# For example, imagine we have one sharded table with 10 partitions, replication
# factor 3 (1 primary and 2 replicas for each shard), 5 nodes and distribute
# data evenly so that each node has 6 shards. In the almost worst case, if node
# A keeps 5 primaries and 1 replica, this node needs 1 + 5*2 + 1 = 12 repslots.
max_replication_slots = 100

# Similar is true for max_wal_senders. Shardlord should have this at equal
# max_replication_slots.

# On worker node 'A', this must be at least 'mws':
# mws = 0
# for each sharded table t
#   for each primary shard of t lying on A
#     mrs += number of this shard replicas
#
# So it is 5*2 = 10 walsenders in previous example.
max_wal_senders = 50

# never set this to 'off' globally while using pg_shardman if you want
# synchronous replication between shards and its replicas.
synchronous_commit = on

# performance-related settings
shared_buffers = 512MB
effective_cache_size = 512MB
work_mem = 4MB
max_connections = 1000
max_wal_size = 5GB

postgres_fdw.use_2pc = on
postgres_fdw.use_repeatable_read = off
